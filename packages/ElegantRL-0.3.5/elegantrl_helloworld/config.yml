DQN:
  # Tuned
  CartPole-v0:
    # agent class
    agent_class: dqn
    # scale reward
    reward_scale: 1
    # discounted rate
    gamma: 0.97
    # target step in exploration
    target_step: 1000
    # dimension of hidden layers
    net_dim: 128
    # number of network layers
    num_layer: 3
    # batch size of the training data
    batch_size: 128
    # update-to-data ratio
    repeat_times: 1
    # explore ratio for epsilon greedy
    explore_rate: 0.25
    eval_gap: 32
    eval_times: 8
    # training steps
    break_step: !!float 8e4
    # gpu id
    learner_gpus: 0
    
  # Tuned
  LunarLander-v2:
    agent_class: dqn
    reward_scale: 1
    gamma: 0.99
    target_step: 1000
    net_dim: 128
    num_layer: 3
    batch_size: 64
    repeat_times: 1
    # exploration noises
    explore_noise: 0.125
    eval_gap: 128
    eval_times: 32
    break_step: !!float 4e5
    learner_gpus: 0
    
DDPG: 
  # Tuned
  Pendulum:
    # agent class name
    agent_class: ddpg
    # scale reward
    reward_scale: 0.5
    # discounted reward
    gamma: 0.97
    target_step: 1000
    net_dim: 128
    batch_size: 128
    repeat_times: 1
    explore_noise: 0.1
    eval_gap: 64
    eval_times: 8
    break_step: !!float 1e5
    learner_gpus: 0

  # Tuned
  LunarLanderContinuous-v2:
    agent_class: ddpg
    reward_scale: 1
    gamma: 0.99
    target_step: 1000
    net_dim: 128
    batch_size: 128
    repeat_times: 1
    explore_noise: 0.1
    eval_gap: 128
    eval_times: 16
    break_step: !!float 4e5
    learner_gpus: 0


  # Tuned
  BipedalWalker-v3:
    agent_class: ddpg
    reward_scale: 0.5
    gamma: 0.99
    target_step: 1000
    net_dim: 256
    num_layer: 3
    batch_size: 128
    repeat_times: 1
    # exploration noises
    explore_noise: 0.1
    eval_gap: 128
    eval_times: 8
    break_step: !!float 1e6
    learner_gpus: 0

PPO:
  # Tuned
  Pendulum:
    agent_class: ppo
    reward_scale: 0.5 
    gamma:  0.97
    target_step:  4000
    net_dim: 128
    num_layer: 2
    batch_size: 256
    repeat_times: 32
    eval_gap: 64
    eval_times: 8
    break_step: !!float 8e5
    learner_gpus: 0
  
  LunarLanderContinuous-v2:
    agent_class: ppo
    gamma:  0.99
    reward_scale: 0.5
    target_step: 4000
    num_layer: 3
    batch_size: 128
    repeat_times: 16
    lambda_entropy: 0.04
    eval_gap: 64
    eval_times: 32
    break_step: !!float 4e5
    learner_gpus: 0

  BipedalWalker-v3:
    agent_class: ppo
    reward_scale: 0.5
    gamma:  0.98
    target_step: 1000
    net_dim:  256
    num_layer:  3
    batch_size: 256
    repeat_times: 16
    eval_gap:  32
    eval_times:  16
    break_step: !!float 1e6
    learner_gpus: 0
