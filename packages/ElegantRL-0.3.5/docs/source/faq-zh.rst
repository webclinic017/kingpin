FAQ


^^^^^^^^
问题 1：在强化学习代码中，对log值裁剪到 -20 到 +2 之间是在进行什么操作？为什么要裁剪到这两个值之间？
^^^^^^^^

在强化学习中，我们举两类对log值进行裁剪的例子：

- 对随机策略的动作的高斯分布的方差的log值 `action_std_log` 进行裁剪

- 对正态分布中对应的概率的log值 (log probability) `logprob` 进行裁剪

简单说，就是相对于正态分布 N~(0, 1) 来说，一个高斯分布的方差的log值如果超过 (-20, +2) 这个区间，那么：

- 如果log值小于 -20，那么这个高斯分布的方差特别小，相当于没有方差，接近于一个确定的数值。

- 如果log值大于 +2，那么这个高斯分布的方差特别大，相当于在接近均值附近是均匀分布。

有空我就展开讲一讲。

-----------------
对随机策略的动作的高斯分布的方差的log值 `action_std_log` 进行裁剪
-----------------
对应代码是  `action_std = self.net_action_std(t_tmp).clip(-20, 2).exp()`, 可以在 `elegantrl/net.py` 里找到。

SAC算法的 `alpha_log` 也能进行类似的裁剪

还可以讲一讲 强化学习里，把权重处理成 log 形式再进行梯度优化。

有空我就展开讲一讲。或者你们来补充（2022-06-08 18:01:54）

-----------------
对正态分布中对应的概率的log值 (log probability) `logprob` 进行裁剪
-----------------
对应代码是  `logprob = logprob.clip(-20, 2)`, 有可能在 `elegantrl/agent/` 里的随机策略梯度算法里找到，因为随机策略梯度算法会用到 `logprob`。

有空我就展开讲一讲。或者你们来补充（2022-06-08 18:01:54）


^^^^^^^^
问题：On-policy 和 off-policy 的区别是什么？
^^^^^^^^
若行为策略和目标策略相同，则是on-policy,若不同则为off-policy

有空我就展开讲一讲。




	
  
  
  
